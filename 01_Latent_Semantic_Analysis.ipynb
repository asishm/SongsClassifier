{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asish\\Documents\\Courses\\Python Based Data Analytics\\Final Project\n",
      "C:\\Users\\Asish\\Documents\\Courses\\Python Based Data Analytics\\Final Project\\Lyrics/2006/\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "curdir = os.path.abspath('')\n",
    "print(curdir)\n",
    "\n",
    "#for j in range(0,10):\n",
    "#    year = ['2006','2007','2008','2009','2010','2011','2012','2013','2014','2015']\n",
    "#    dHome = \"/Users/HimanshuBharara/Documents/CU-Sem2/IEORE4571/Projects/Lyrics/\"+year[j]+\"/\"\n",
    "#    print(dHome)\n",
    "\n",
    "dHome = os.path.join(curdir,\"Lyrics/2006/\")\n",
    "print(dHome)\n",
    "lyrics_array = []\n",
    "print(lyrics_array)\n",
    "for i in range(0,100):\n",
    "        try:\n",
    "            name=dHome+str(i)+\".\"+\"txt\"\n",
    "            file=open(name,'r+')\n",
    "            lyrics_array.append(file.read())\n",
    "            #globals()['string%s' % i] = file.read()\n",
    "            file.close()\n",
    "        except ValueError:\n",
    "            print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dHome = os.path.join(curdir, \"Lyrics/2006/\")\n",
    "name= os.path.join(curdir, \"Lyrics/2006/74.txt\")\n",
    "file=open(name,'r+')\n",
    "#print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Counts number of occurences of each word in a string of words\n",
    "from collections import Counter\n",
    "\n",
    "#lyrics_array = ['Julie loves me more than Linda loves me',\n",
    "#'Jane likes me more than Julie loves me',\n",
    "#'He likes basketball more than baseball']\n",
    "\n",
    "#words = string0.split()\n",
    "#wordCount = Counter(words)\n",
    "#print(wordCount)\n",
    "for doc in lyrics_array:\n",
    "    tf = Counter()\n",
    "    for word in doc.split():\n",
    "        tf[word] +=1\n",
    "\n",
    "#print(tf.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All combined, here is our master document term matrix:\n"
     ]
    }
   ],
   "source": [
    "import string #allows for format()\n",
    "    \n",
    "def build_lexicon(corpus):\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "\n",
    "def tf(term, document):\n",
    "  return freq(term, document)\n",
    "\n",
    "def freq(term, document):\n",
    "  return document.split().count(term)\n",
    "\n",
    "vocabulary = build_lexicon(lyrics_array)\n",
    "\n",
    "doc_term_matrix = []\n",
    "#print ('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "for doc in lyrics_array:\n",
    "    #print ('The doc is \"' + doc + '\"')\n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    #print ('The tf vector for Document %d is [%s]' % ((lyrics_array.index(doc)+1), tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "    \n",
    "    # here's a test: why did I wrap mydoclist.index(doc)+1 in parens?  it returns an int...\n",
    "    # try it!  type(mydoclist.index(doc) + 1)\n",
    "\n",
    "print (\"All combined, here is our master document term matrix:\")\n",
    "#print (doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A regular old document term matrix: \n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "\n",
      "A document term matrix with row-wise L2 norms of 1:\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el**2 for el in vec])\n",
    "    if denom == 0:\n",
    "        return [(0) for el in vec]\n",
    "    else:\n",
    "        return [(el / math.sqrt(denom)) for el in vec]\n",
    "    \n",
    "doc_term_matrix_l2 = []\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "\n",
    "print ('A regular old document term matrix: ')\n",
    "print (np.matrix(doc_term_matrix))\n",
    "print ('\\nA document term matrix with row-wise L2 norms of 1:')\n",
    "print (np.matrix(doc_term_matrix_l2))\n",
    "\n",
    "# if you want to check this math, perform the following:\n",
    "# from numpy import linalg as la\n",
    "# la.norm(doc_term_matrix[0])\n",
    "# la.norm(doc_term_matrix_l2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5887\n"
     ]
    }
   ],
   "source": [
    "def numDocsContaining(word, doclist):\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc) > 0:\n",
    "            doccount +=1\n",
    "    return doccount \n",
    "\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist)\n",
    "    df = numDocsContaining(word, doclist)\n",
    "    return np.log(n_samples / 1+df)\n",
    "\n",
    "my_idf_vector = [idf(word, lyrics_array) for word in vocabulary]\n",
    "\n",
    "print(len(vocabulary))\n",
    "#print ('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "#print ('The inverse document frequency vector is [' + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat, idf_vector)\n",
    "    return idf_mat\n",
    "\n",
    "my_idf_matrix = build_idf_matrix(my_idf_vector)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_tfidf = []\n",
    "dHome = curdir\n",
    "#performing tf-idf matrix multiplication\n",
    "for tf_vector in doc_term_matrix:\n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
    "\n",
    "#normalizing\n",
    "doc_term_matrix_tfidf_l2 = []\n",
    "for tf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "                                    \n",
    "#print vocabulary\n",
    "i=2006\n",
    "print(np.matrix(doc_term_matrix_tfidf_l2)) # np.matrix() just to make it easier to look at\n",
    "dic_t = dHome+str(i)+\"_\"+\"dict\"+\".txt\"\n",
    "vec=dHome+str(i)+\"_\"+\"vector\"+\".\"+\"txt\"\n",
    "#file=open(dic_t,'a')\n",
    "\n",
    "#np.savetxt(vec,np.matrix(doc_term_matrix_tfidf_l2))\n",
    "#file.write(list(vocabulary))\n",
    "print(\"done\")\n",
    "#file.write(str(vocabulary))\n",
    "#file.write(print(np.matrix(doc_term_matrix_tfidf_l2)))\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Understanding Wordnet and Implementing Basic Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "list1 = vocabulary\n",
    "list2 = ['Anger', 'Surprise', 'Joy', 'Sadness', 'love', 'Fear']\n",
    "\n",
    "list3 = []\n",
    "\n",
    "for i,word1 in enumerate(list1):\n",
    "    k = []\n",
    "    for word2 in list2:\n",
    "        wordFromList1 = wordnet.synsets(word1)\n",
    "        wordFromList2 = wordnet.synsets(word2)\n",
    "        if wordFromList1 and wordFromList2: #Thanks to @alexis' note\n",
    "            s = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "            k.append(s)\n",
    "    list3.append(k)\n",
    "    print(word1, k)\n",
    "    if i == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.36363636363636365, 0.3333333333333333, 0.3333333333333333], [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.36363636363636365, 0.3333333333333333, 0.3333333333333333], [None, None, None, None, None, None], [0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.46153846153846156, 0.42857142857142855, 0.42857142857142855], []]\n"
     ]
    }
   ],
   "source": [
    "b = np.array(list3)\n",
    "print(list3[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(3, 11)\n",
      "(2, 6)\n"
     ]
    }
   ],
   "source": [
    "print(type(doc_term_matrix_tfidf))\n",
    "a1 = np.matrix(doc_term_matrix_tfidf)\n",
    "a2 = np.matrix(list3)\n",
    "\n",
    "print(a1.shape)\n",
    "print(a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "list1 = ['Compare', 'require']\n",
    "list2 = ['Anger', 'Surprise', 'Joy', 'Sadness', 'love', 'Fear']\n",
    "allsyns1 = set(ss for word in list1 for ss in wordnet.synsets(word))\n",
    "allsyns2 = set(ss for word in list2 for ss in wordnet.synsets(word))\n",
    "print(allsyns1)\n",
    "print(allsyns2)\n",
    "best = max((wordnet.wup_similarity(s1, s2) or 0, s1, s2) for s1, s2 in product(allsyns1, allsyns2))\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list1 = ['Compare', 'require']\n",
    "list2 = ['Anger', 'Surprise', 'Joy', 'Sadness', 'love', 'Fear']\n",
    "for word in list2:\n",
    "    for ss in wordnet.synsets(word):\n",
    "        print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
