{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/HimanshuBharara/Documents/CU-Sem2/IEORE4571/Projects/\n",
      "/Users/HimanshuBharara/Documents/CU-Sem2/IEORE4571/Projects/Lyrics/2040/\n",
      "[]\n",
      "All combined, here is our master document term matrix:\n",
      "A regular old document term matrix: \n",
      "\n",
      "A document term matrix with row-wise L2 norms of 1:\n",
      "117\n",
      "tfidf done, shape is: (1, 117)\n",
      "(117, 6) count of zeros: 44\n",
      "2040 done\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Created-Date: 04/14/2016                            #\n",
    "# Last Modified: 04/27/2016                           #\n",
    "# Created by: Himanshu Bharara , Asish Mahapatra      #\n",
    "#                                                     #\n",
    "# Objective: Calculate the TF-IDF vector              #\n",
    "# file by taking lyrics of 100 songs for each year    #\n",
    "# sequentially                                        #\n",
    "#                                                     #\n",
    "# Instruction before running:                         #\n",
    "# 1. Please change the file path of directory         # \n",
    "# 2. change ranges based on the number of lyrics      # \n",
    "#    you have in the directory                        #  \n",
    "# 3. Please follow similar nomenclature as used in the# \n",
    "#    code                                             #\n",
    "#######################################################\n",
    "\n",
    "\n",
    "\n",
    "import os.path\n",
    "import string #allows for format()\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def main(year):\n",
    "        \n",
    "        print(curdir)  # Print Home Directory\n",
    "\n",
    "        dHome = os.path.join(curdir,\"Lyrics/{}/\".format(year))\n",
    "        print(dHome)\n",
    "        lyrics_array = []\n",
    "        print(lyrics_array)\n",
    "        for i in range(0,1):    # Extract all the lyrics of the song for the given year\n",
    "                try:\n",
    "                    name=dHome+str(i)+\".txt\"\n",
    "                    file=open(name,'r+')\n",
    "                    lyrics_array.append(file.read())\n",
    "                    file.close()\n",
    "                except ValueError:\n",
    "                    print(i)\n",
    "\n",
    "        dHome = os.path.join(curdir, \"Lyrics/{}/\".format(year))\n",
    "        file=open(name,'r+')\n",
    "\n",
    "        df = {}\n",
    "        tf = {}\n",
    "\n",
    "        for i,doc in enumerate(lyrics_array):\n",
    "            for word in doc.split():\n",
    "                k = word\n",
    "                word = word.lower().strip()  # Convert words to lower case and then strip words from sentence to create an array\n",
    "                word = re.sub(r'[^a-z]', '', word) # Use Regular Expression to remove apostrophes and other puncutation or semantic errors\n",
    "                if not tf.get(word):\n",
    "                        temp_list = []\n",
    "                        for j in range(0,1):\n",
    "                                if j == i:\n",
    "                                        temp_list.append([i,1])\n",
    "                                else:\n",
    "                                        temp_list.append([j,0])\n",
    "                        tf[word] = temp_list\n",
    "                else:\n",
    "                        tf[word][i] = [i, tf[word][i][1]+1]\n",
    "                df[word] = df.get(word, []) + [k]\n",
    "                \n",
    "        #del df['']\n",
    "        #del tf['']\n",
    "        vocabulary = list(df.keys())\n",
    "\n",
    "        doc_term_matrix = []\n",
    "        for doc in range(0,1):                   # Build vocabulary from the entire 100 lyrics file to create a N size vector\n",
    "                tf_vector = [tf[word][doc][1] for word in vocabulary]\n",
    "                doc_term_matrix.append(tf_vector)\n",
    "\n",
    "        print (\"All combined, here is our master document term matrix:\")\n",
    "\n",
    "        def l2_normalizer(vec):                  # Normalize the 100 X N vector by dividing the Term Frequency / Document Frequency\n",
    "            denom = np.sum([el**2 for el in vec])\n",
    "            if denom == 0:\n",
    "                return [(0) for el in vec]\n",
    "            else:\n",
    "                return [(el / math.sqrt(denom)) for el in vec]\n",
    "           \n",
    "        doc_term_matrix_l2 = []\n",
    "        for vec in doc_term_matrix:\n",
    "            doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "\n",
    "        print ('A regular old document term matrix: ')\n",
    "        print ('\\nA document term matrix with row-wise L2 norms of 1:')\n",
    "\n",
    "\n",
    "        def numDocsContaining(word, doclist):\n",
    "            doccount = 0\n",
    "            for element in tf[word]:\n",
    "                    if element[1] > 0:\n",
    "                            doccount += 1\n",
    "            return doccount \n",
    "\n",
    "        def idf(word, doclist):\n",
    "            n_samples = len(doclist)\n",
    "            df = numDocsContaining(word, doclist)\n",
    "            return np.log(n_samples / 1+df)\n",
    "\n",
    "        my_idf_vector = [idf(word, lyrics_array) for word in vocabulary]\n",
    "\n",
    "        print(len(vocabulary))\n",
    "\n",
    "        def build_idf_matrix(idf_vector):\n",
    "            idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
    "            np.fill_diagonal(idf_mat, idf_vector)\n",
    "            return idf_mat\n",
    "\n",
    "        my_idf_matrix = build_idf_matrix(my_idf_vector)\n",
    "\n",
    "        doc_term_matrix_tfidf = []\n",
    "        #performing tf-idf matrix multiplication\n",
    "        for tf_vector in doc_term_matrix:\n",
    "            doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
    "\n",
    "        #normalizing\n",
    "        doc_term_matrix_tfidf_l2 = []\n",
    "        for tf_vector in doc_term_matrix_tfidf:\n",
    "            doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "\n",
    "        a = np.array(doc_term_matrix_tfidf_l2)\n",
    "        print(\"tfidf done, shape is: {}\".format(a.shape))\n",
    "                                \n",
    "        list1 = vocabulary\n",
    "        list2 = ['anger', 'surprise', 'joy', 'sadness', 'love', 'fear']\n",
    "\n",
    "        list3 = []\n",
    "\n",
    "        for i,word1 in enumerate(list1):\n",
    "            k = []\n",
    "            for word2 in list2:\n",
    "                wordFromList1 = wordnet.synsets(word1)\n",
    "##                wordFromList2 = wordnet.synsets(word2)\n",
    "                wordFromList2n = wordnet.synsets(word2, pos = 'n')\n",
    "                wordFromList2v = wordnet.synsets(word2, pos = 'v')\n",
    "                q,s = 0,0\n",
    "                if wordFromList1 and wordFromList2n: \n",
    "                    s = max(syn.wup_similarity(wordFromList2n[0]) if syn.wup_similarity(wordFromList2n[0])\n",
    "                         is not None else 0 for syn in wordFromList1)\n",
    "\n",
    "##                if wordFromList1 and wordFromList2:\n",
    "##                    s = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "##                    if s == None:\n",
    "##                        k.append(0.)\n",
    "##                        continue\n",
    "##                    k.append(s)\n",
    "                if wordFromList1 and wordFromList2v: #Thanks to @alexis' note\n",
    "                    q = max(syn.wup_similarity(wordFromList2v[0]) if syn.wup_similarity(wordFromList2v[0])\n",
    "                         is not None else 0 for syn in wordFromList1)\n",
    "\n",
    "                k.append(max(q,s))\n",
    "            if k == [] or k == [None, None, None, None, None, None]:\n",
    "                k = [0., 0., 0., 0., 0., 0.]\n",
    "            list3.append(k)\n",
    "\n",
    "        count = 0\n",
    "        for row in list3:\n",
    "                if row == [0, 0, 0, 0, 0, 0]:\n",
    "                        count += 1\n",
    "        b = np.array(list3)\n",
    "        print(b.shape, \"count of zeros:\", count)\n",
    "\n",
    "        output = np.dot(doc_term_matrix_tfidf_l2, b)\n",
    "        return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    years = range(2040,2041)\n",
    "    curdir = r\"/Users/HimanshuBharara/Documents/CU-Sem2/IEORE4571/Projects/\"\n",
    "    for year in years:\n",
    "        result = main(year)\n",
    "        with open(os.path.join(curdir, \"{}_clean1.csv\".format(year)), \"w\", newline = '') as f:\n",
    "            a = csv.writer(f)\n",
    "            for row in result:\n",
    "                a.writerow(row)\n",
    "        print(\"{} done\".format(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
